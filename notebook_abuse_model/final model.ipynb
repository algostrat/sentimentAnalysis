{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sb\n",
    "from nltk.corpus import stopwords\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import unidecode\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize \n",
    "import matplotlib.animation as animation\n",
    "import operator\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train_E6oV3lV.csv')\n",
    "df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n",
    "\n",
    "#default smaller dictionary\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "\n",
    "bigram_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "# term_index is the column of the term and count_index is the\n",
    "# column of the term frequency\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "\n",
    "#Stemming\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Lemmitization\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(s):\n",
    "    \"\"\"\n",
    "    s is the string/tweet\n",
    "    manipulate s and return it\n",
    "    Tfidfvectorizer's preprocessor should .apply this function to every tweet/string    \n",
    "    \"\"\"\n",
    "    \n",
    "    #removing numbers this isn't working (this will leave extra spaces if digit is enclosed by two spaces)\n",
    "    s = ''.join([i for i in s if not i.isdigit()])\n",
    "    \n",
    "    #Removing all the greek characters using unidecode library\n",
    "    s = ' '.join([unidecode.unidecode(word) for word in s.split()])\n",
    "        \n",
    "    #removing mentions @\n",
    "    s = ' '.join([word for word in s.split()if not word.startswith(\"@\")])\n",
    "    \n",
    "    #Removing the word 'hmm' and it's variants\n",
    "    s = ' '.join([word for word in s.split() if not word == 'h(m)+' ])\n",
    "    \n",
    "    #removing extra spaces/tabs\n",
    "    s =  ' '.join(s.split())\n",
    "    \n",
    "    #Code for removing slang words\n",
    "    d = {'luv':'love','wud':'would','lyk':'like','wateva':'whatever','ttyl':'talk to you later',\n",
    "               'kul':'cool','fyn':'fine','omg':'oh my god!','fam':'family','bruh':'brother',\n",
    "               'cud':'could','fud':'food','lol':'laugh out loud', 'wtf':'what the fuck','wyd':'what are you doing',\n",
    "                'wdym':'what do you mean','lmao':'laugh my ass off','fml':'fuck my life','np':'no problem',\n",
    "                'ffs':'for fucks sake','nvm':'nevermind','bro':'brother','bra':'brother','tldr':'too long, didn\\'t read',\n",
    "                'stfu':'shut the fuck up', 'tbh':'to be honest','idek':'i don\\'t even know',\n",
    "                'diy': 'Do it yourself','rn': 'right now', 'btw':'by the way','u':'you' ,'imo': 'in my opinion', 'ily':'i love you',\n",
    "    'bf':'boyfriend','gf':'girlfriend','5g':'5th generation','tldr':'too long didn\\'t read', 'rofl':'rolling on the floor laughing',\n",
    "    'lmk':'let me know', 'hmu':'hit me up','tba':'to be announced','asap':'as soon as possible','roi':'return on investment',\n",
    "     'tgif':'thank goodness it\\'s friday'} ## Need a huge dictionary\n",
    "    \n",
    "    s = ' '.join(d[word] if word in d else word for word in s.split())\n",
    "    \n",
    "    #start of new preprocessing stuff\n",
    "    \n",
    "    #removing hashtags\n",
    "    s = ' '.join([word.replace('#', '') for word in s.split()])\n",
    "    \n",
    "    #removes links\n",
    "    s = ' '.join([re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",word) for word in s.split()])\n",
    "    \n",
    "    #reduce excess letters ie finallllly -> finally\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    s = ' '.join([pattern.sub(r\"\\1\\1\", word) for word in s.split()])\n",
    "    \n",
    "    #autocorrect (must run cell above)\n",
    "    s = ''.join([word.term for word in sym_spell.lookup_compound(s, max_edit_distance=2)])\n",
    "    \n",
    "    #removing single letters except single digits or i or a (perform autocorrect first)\n",
    "    s = ' '.join( [w for w in s.split() if (len(w)>1 or w.isdigit() or w.lower()=='a' or w.lower() =='i')] )\n",
    "    \n",
    "    #remove stop words \n",
    "    s = ' '.join([word for word in s.split() if not word in set(stopwords.words('english'))])\n",
    "    \n",
    "    #lemmatization\n",
    "    s = ' '.join([lemmatizer.lemmatize(word) for word in s.split()])\n",
    "\n",
    "    #stemming\n",
    "    s = ' '.join([ps.stem(word) for word in s.split()])\n",
    "    \n",
    "    return s\n",
    "    \n",
    "\n",
    "string = \"hello world finallly  helllo 44 my name is @@ # ** and s 4 my d1 0 11og's name is **\"\n",
    "\n",
    "print(string)\n",
    "print(preproc(string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframed and corpus1 and courpus2\n",
    "data_frame.to_pickle('data/dataframe.pkl')\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
