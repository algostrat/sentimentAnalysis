{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I have removed all greek characters, @,numbers.\n",
    "#Also I have removed 'hmm' word and it's variants\n",
    "#Creating a dictionary to map words such as luv to love, wud to would.Need more suggestions on this.nyc:nice\n",
    "#Removed stop words\n",
    "#Performed lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sb\n",
    "from nltk.corpus import stopwords\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import unidecode\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize \n",
    "import matplotlib.animation as animation\n",
    "import operator\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train_E6oV3lV.csv')\n",
    "df.drop_duplicates(inplace = True)\n",
    "df['clean_tweet'] = df['tweet'].apply(lambda x : ' '.join([tweet for tweet in x.split()if not tweet.startswith(\"@\")]))\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x : ' '.join([tweet for tweet in x.split() if not tweet == '\\d*']))\n",
    "#Removing all the greek characters using unidecode library\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x : ' '.join([unidecode.unidecode(word) for word in x.split()])) \n",
    "#Removing the word 'hmm' and it's variants\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x : ' '.join([word for word in x.split() if not word == 'h(m)+' ]))\n",
    "\n",
    "#Code for removing slang words\n",
    "d = {'luv':'love','wud':'would','lyk':'like','wateva':'whatever','ttyl':'talk to you later',\n",
    "               'kul':'cool','fyn':'fine','omg':'oh my god!','fam':'family','bruh':'brother',\n",
    "               'cud':'could','fud':'food','lol':'laugh out loud', 'wtf':'what the fuck','wyd':'what are you doing',\n",
    "                'wdym':'what do you mean','lmao':'laugh my ass off','fml':'fuck my life','np':'no problem',\n",
    "                'ffs':'for fucks sake','nvm':'nevermind','bro':'brother','bra':'brother','tldr':'too long, didn\\'t read',\n",
    "                'stfu':'shut the fuck up', 'tbh':'to be honest','idek':'i don\\'t even know',\n",
    "                'diy': 'Do it yourself','rn': 'right now', 'btw':'by the way', 'imo': 'in my opinion', 'ily':'i love you',\n",
    "    'bf':'boyfriend','gf':'girlfriend','5g':'5th generation','tldr':'too long didn\\'t read', 'rofl':'rolling on the floor laughing',\n",
    "    'lmk':'let me know', 'hmu':'hit me up','tba':'to be announced','asap':'as soon as possible','roi':'return on investment','tgif':'thank goodness it\\'s friday'} ## Need a huge dictionary\n",
    "words = \"I luv myself\"\n",
    "words = words.split()\n",
    "reformed = [d[word] if word in d else word for word in words]\n",
    "reformed = \" \".join(reformed)\n",
    "\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x : ' '.join(d[word] if word in d else word for word in x.split()))\n",
    "\n",
    "#Finding words with # attached to it\n",
    "df['#'] = df['clean_tweet'].apply(lambda x : ' '.join([word for word in x.split() if word.startswith('#')]))\n",
    "\n",
    "frame = df['#']\n",
    "\n",
    "frame = pd.DataFrame(frame)\n",
    "\n",
    "frame = frame.rename({'#':'Count(#)'},axis = 'columns')\n",
    "\n",
    "frame[frame['Count(#)'] == ''] = 'No hashtags'\n",
    "\n",
    "data_frame = pd.concat([df,frame],axis = 1)\n",
    "\n",
    "data_frame.drop('#',axis = 1,inplace = True)\n",
    "\n",
    "#Column showing whether the corresponding tweet has a hash tagged word or not\n",
    "data_frame = data_frame.rename({'Count(#)':'Hash words'},axis = 'columns')\n",
    "\n",
    "#remove hashtags\n",
    " # remove hashtags only\n",
    "data_frame['clean_tweet'] = data_frame['clean_tweet'].apply(lambda x : ' '.join([word.replace('#', '') for word in x.split()]))\n",
    "\n",
    "#removing links\n",
    "data_frame['clean_tweet'] = data_frame['clean_tweet'].apply(lambda x : ' '.join([re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",word) for word in x.split()]))\n",
    "\n",
    "#reduce excess letters\n",
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "#print reduce_lengthening( \"finallllllly\" )\n",
    "data_frame['clean_tweet'] = data_frame['clean_tweet'].apply(lambda x : ' '.join([reduce_lengthening(word) for word in x.split()]))\n",
    "\n",
    "#removing single letters except single digits\n",
    "#re.sub(r'(?:^| )\\w(?:$| )', ' ', text).strip() #removes digits too\n",
    "data_frame['clean_tweet'] = data_frame['clean_tweet'].apply(lambda x : ' '.join( [w for w in x.split() if (len(w)>1 or w.isdigit())] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train_E6oV3lV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data doesn't contain duplicate values neither does it contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to remove @\n",
    "df['clean_tweet'] = df['tweet'].apply(lambda x : ' '.join([tweet for tweet in x.split()if not tweet.startswith(\"@\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing numbers\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x : ' '.join([tweet for tweet in x.split() if not tweet == '\\d*']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing all the greek characters using unidecode library\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x : ' '.join([unidecode.unidecode(word) for word in x.split()])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check the disappearance of greek symbols\n",
    "df['clean_tweet'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the word 'hmm' and it's variants\n",
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x : ' '.join([word for word in x.split() if not word == 'h(m)+' ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for removing slang words\n",
    "d = {'luv':'love','wud':'would','lyk':'like','wateva':'whatever','ttyl':'talk to you later',\n",
    "               'kul':'cool','fyn':'fine','omg':'oh my god!','fam':'family','bruh':'brother',\n",
    "               'cud':'could','fud':'food','lol':'laugh out loud', 'wtf':'what the fuck','wyd':'what are you doing',\n",
    "                'wdym':'what do you mean','lmao':'laugh my ass off','fml':'fuck my life','np':'no problem',\n",
    "                'ffs':'for fucks sake','nvm':'nevermind','bro':'brother','bra':'brother','tldr':'too long, didn\\'t read',\n",
    "                'stfu':'shut the fuck up', 'tbh':'to be honest','idek':'i don\\'t even know',\n",
    "                'diy': 'Do it yourself','rn': 'right now', 'btw':'by the way', 'imo': 'in my opinion', 'ily':'i love you',\n",
    "    'bf':'boyfriend','gf':'girlfriend','5g':'5th generation','tldr':'too long didn\\'t read', 'rofl':'rolling on the floor laughing',\n",
    "    'lmk':'let me know', 'hmu':'hit me up','tba':'to be announced','asap':'as soon as possible','roi':'return on investment','tgif':'thank goodness it\\'s friday'} ## Need a huge dictionary\n",
    "words = \"I luv myself\"\n",
    "words = words.split()\n",
    "reformed = [d[word] if word in d else word for word in words]\n",
    "reformed = \" \".join(reformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweet'] = df['clean_tweet'].apply(lambda x : ' '.join(d[word] if word in d else word for word in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding words with # attached to it\n",
    "df['#'] = df['clean_tweet'].apply(lambda x : ' '.join([word for word in x.split() if word.startswith('#')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = df['#']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = frame.rename({'#':'Count(#)'},axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame[frame['Count(#)'] == ''] = 'No hashtags'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.concat([df,frame],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.drop('#',axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column showing whether the corresponding tweet has a hash tagged word or not\n",
    "data_frame = data_frame.rename({'Count(#)':'Hash words'},axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove hashtags\n",
    " # remove hashtags only\n",
    "data_frame['clean_tweet'] = data_frame['clean_tweet'].apply(lambda x : ' '.join([word.replace('#', '') for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove links\n",
    "#re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text)\n",
    "test = data_frame[0:7]\n",
    "test[\"clean_tweet\"][0] = \"New 12\\\" https://t.co/Xj1e5qHVxP\"\n",
    "#test\n",
    "test['clean_tweet'] = test['clean_tweet'].apply(lambda x : ' '.join([re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",word) for word in x.split()]))\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing links\n",
    "data_frame['clean_tweet'] = data_frame['clean_tweet'].apply(lambda x : ' '.join([re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce excess letters\n",
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "#print reduce_lengthening( \"finallllllly\" )\n",
    "data_frame['clean_tweet'] = data_frame['clean_tweet'].apply(lambda x : ' '.join([reduce_lengthening(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing single letters except single digits\n",
    "#re.sub(r'(?:^| )\\w(?:$| )', ' ', text).strip() #removes digits too\n",
    "data_frame['clean_tweet'] = data_frame['clean_tweet'].apply(lambda x : ' '.join( [w for w in x.split() if (len(w)>1 or w.isdigit())] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT RUN - TOO SLOW!!! -- autocorrect -- TEST THE ACCURACY OF THIS\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm_notebook().pandas()\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "word = \"plumming\"\n",
    "print(spell.correction(word))\n",
    "print(spell.word_probability(word))\n",
    "data_frame['clean_tweet'] = data_frame['clean_tweet'].progress_apply(lambda x : ' '.join([spell.correction(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where is the love he had dated for much of the past who couldn't read in six grade and inspired him\n",
      "where is the love he had dated for much of the past who couldn't read in six grade and inspired him\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>Hash words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>whereis th elove hehad dated forImuch of thepa...</td>\n",
       "      <td>#run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks for left credit can use cause they don ...</td>\n",
       "      <td>#lyft #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>by day your majesty</td>\n",
       "      <td>No hashtags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>model love take with all the time in ord do</td>\n",
       "      <td>#model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>fact guide society now motivation</td>\n",
       "      <td>#motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
       "      <td>a a huge fan fare and big talking before they ...</td>\n",
       "      <td>#allshowandnogo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
       "      <td>camping tomorrow danny</td>\n",
       "      <td>No hashtags</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1      0   @user when a father is dysfunctional and is s...   \n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3      0                                bihday your majesty   \n",
       "3   4      0  #model   i love u take with u all the time in ...   \n",
       "4   5      0             factsguide: society now    #motivation   \n",
       "5   6      0  [2/2] huge fan fare and big talking before the...   \n",
       "6   7      0   @user camping tomorrow @user @user @user @use...   \n",
       "\n",
       "                                         clean_tweet  \\\n",
       "0  whereis th elove hehad dated forImuch of thepa...   \n",
       "1  thanks for left credit can use cause they don ...   \n",
       "2                                by day your majesty   \n",
       "3        model love take with all the time in ord do   \n",
       "4                  fact guide society now motivation   \n",
       "5  a a huge fan fare and big talking before they ...   \n",
       "6                             camping tomorrow danny   \n",
       "\n",
       "                       Hash words  \n",
       "0                            #run  \n",
       "1  #lyft #disapointed #getthanked  \n",
       "2                     No hashtags  \n",
       "3                          #model  \n",
       "4                     #motivation  \n",
       "5                 #allshowandnogo  \n",
       "6                     No hashtags  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "#tqdm_notebook().pandas()\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "bigram_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "# term_index is the column of the term and count_index is the\n",
    "# column of the term frequency\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "\n",
    "# lookup suggestions for multi-word input strings (supports compound\n",
    "# splitting & merging)\n",
    "input_term = (\"whereis th elove hehad dated forImuch of thepast who \"\n",
    "              \"couldnt read in sixtgrade and ins pired him\")\n",
    "# max edit distance per lookup (per single word, not per whole input string)\n",
    "suggestions = sym_spell.lookup_compound(input_term, max_edit_distance=2)\n",
    "# display suggestion term, edit distance, and term frequency\n",
    "\n",
    "for suggestion in suggestions:\n",
    "    print(suggestion.term)\n",
    "\n",
    "print(suggestions[0].term)\n",
    "    \n",
    "test = data_frame[0:7]\n",
    "test[\"clean_tweet\"][0] = input_term\n",
    "#test['clean_tweet'] = test['clean_tweet'].progress_apply(lambda x : ''.join([word.term for word in sym_spell.lookup_compound(x, max_edit_distance=2)]))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stopwords\n",
    "data_frame['clean_tweet'] = data_frame['clean_tweet'].apply(lambda x : ' '.join([word for word in x.split() if not word in set(stopwords.words('english'))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing anything that's not an englsih word while ignoring digits\n",
    "# imay not incorporate this further dependent on whether it performs well or not\n",
    "# check for cursewords\n",
    "import nltk \n",
    "from better_profanity import profanity #\n",
    "#nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\" \".join(w for w in nltk.wordpunct_tokenize(sent) if w.lower() in words or not w.isalpha())\n",
    "data_frame['clean_tweet'][0] = \"man fuck you sdf sdg  you racist peice of shit i hope you die of aids you fucker\"\n",
    "data_frame['clean_tweet'] = data_frame['clean_tweet'].apply(lambda x : ' '.join(w for w in nltk.wordpunct_tokenize(x) if w.lower() in words or not w.isalpha() or profanity.contains_profanity(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>Hash words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>man fuck you you racist of shit i hope you die...</td>\n",
       "      <td>#run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks credit can use cause offer</td>\n",
       "      <td>#lyft #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>majesty</td>\n",
       "      <td>No hashtags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>model love take time urd</td>\n",
       "      <td>#model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>society motivation</td>\n",
       "      <td>#motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1      0   @user when a father is dysfunctional and is s...   \n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3      0                                bihday your majesty   \n",
       "3   4      0  #model   i love u take with u all the time in ...   \n",
       "4   5      0             factsguide: society now    #motivation   \n",
       "\n",
       "                                         clean_tweet  \\\n",
       "0  man fuck you you racist of shit i hope you die...   \n",
       "1                  thanks credit can use cause offer   \n",
       "2                                            majesty   \n",
       "3                           model love take time urd   \n",
       "4                                 society motivation   \n",
       "\n",
       "                       Hash words  \n",
       "0                            #run  \n",
       "1  #lyft #disapointed #getthanked  \n",
       "2                     No hashtags  \n",
       "3                          #model  \n",
       "4                     #motivation  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmitization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data_frame['clean_tweet'] = data_frame['clean_tweet'].apply(lambda x : ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "ps = PorterStemmer()\n",
    "adwait = data_frame\n",
    "#adwait.head()\n",
    "data_frame['clean_tweet'] = data_frame['clean_tweet'].apply(lambda x : ' '.join([ps.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "corpus = []\n",
    "for i in range(0,31962):\n",
    "    tweet = data_frame['clean_tweet'][i]\n",
    "    tweet = tweet.lower()\n",
    "    tweet = tweet.split()\n",
    "    tweet = [ps.stem(word) for word in tweet if not word in set(stopwords.words('english'))]\n",
    "    tweet = ' '.join(tweet)\n",
    "    corpus.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensuring all the tweets are tokenized into individual words\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_words = ' '.join([word for word in data_frame['clean_tweet'][data_frame['label'] == 0]])\n",
    "wordcloud = WordCloud(width = 800, height = 500, max_font_size = 110,max_words = 100).generate(normal_words)\n",
    "print('Normal words')\n",
    "plt.figure(figsize= (12,8))\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear',cmap='viridis')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_words = ' '.join([word for word in data_frame['clean_tweet'][data_frame['label'] == 1]])\n",
    "wordcloud = WordCloud(width = 800, height = 500, max_font_size = 110,max_words = 100).generate(normal_words)\n",
    "print('Normal words')\n",
    "plt.figure(figsize= (12,8))\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting positive hashtags\n",
    "\n",
    "hash_positive = []\n",
    "hash_negative = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_extract(x):\n",
    "    hashtags = []\n",
    "    # Loop over the words in the tweet\n",
    "    for i in x:\n",
    "        ht = re.findall(r\"#(\\w+)\", i)\n",
    "        hashtags.append(ht)\n",
    "\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_positive = hashtag_extract(data_frame['clean_tweet'][data_frame['label'] == 0])\n",
    "\n",
    "# extracting hashtags from racist/sexist tweets\n",
    "hash_negative = hashtag_extract(data_frame['clean_tweet'][data_frame['label'] == 1])\n",
    "\n",
    "# Converting a multidimensional list to a 1-D list\n",
    "hash_positive = sum(hash_positive,[])\n",
    "hash_negative = sum(hash_negative,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "q = Counter(hash_positive)\n",
    "q = dict(q.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_positive_count = list(q.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_positive_count[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Counter(hash_negative)\n",
    "r = dict(r.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_negative_count = list(r.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_negative_count[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_positive_values = list(q.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_positive_values[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_negative_values = list(r.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_negative_values[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe to represent top 20 positive and negative hash words\n",
    "l1 = pd.DataFrame(l_positive_values[0:20],columns = ['Positive_Words'])\n",
    "l2 = pd.DataFrame(l_positive_count[0:20],columns = ['Positive_Count'])\n",
    "l3 = pd.DataFrame(l_negative_values[0:20],columns = ['Negative_Words'])\n",
    "l4 = pd.DataFrame(l_negative_count[0:20],columns = ['Negative_Count'])\n",
    "z = pd.concat([l1,l2,l3,l4],axis = 1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Animated plot for positive words with their frequency\n",
    "fig = px.bar(z, x=\"Positive_Words\", y=\"Positive_Count\",animation_frame=\"Positive_Count\",\n",
    "            hover_name=\"Positive_Words\")\n",
    "fig.layout.updatemenus[0].buttons[0].args[1][\"frame\"][\"duration\"] = 1200\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animated plot for negative words with their frequency\n",
    "fig = px.bar(z, x=\"Negative_Words\", y=\"Negative_Count\",animation_frame=\"Negative_Count\",\n",
    "            hover_name=\"Negative_Words\")\n",
    "fig.layout.updatemenus[0].buttons[0].args[1][\"frame\"][\"duration\"] = 1200\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normal histogram of positive words\n",
    "fig = px.bar(z, x=\"Positive_Words\", y=\"Positive_Count\",\n",
    "            hover_name=\"Positive_Words\",color = 'Positive_Count')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normal histogram of negative words\n",
    "fig = px.bar(z, x=\"Negative_Words\", y=\"Negative_Count\",\n",
    "            hover_name=\"Negative_Words\",color= 'Negative_Count')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Techniques to convert the tweets into Bag-of-Words, TF-IDF, and Word Embeddings\n",
    "#Building various classifiers: -\n",
    "#TF-IDF approach\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2,stop_words='english')\n",
    "# TF-IDF feature matrix\n",
    "X1 = tfidf_vectorizer.fit_transform(corpus).toarray()\n",
    "Y1 = df.loc[:,'label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X1_train, X1_test, Y1_train, Y1_test = train_test_split(data_frame['clean_tweet'], data_frame['label'], test_size = 0.3, random_state=0, shuffle = True, stratify=data_frame['label'])\n",
    "vectorizer = TfidfVectorizer()\n",
    "X1_train_vect = vectorizer.fit_transform(X1_train)\n",
    "Y1 = df.loc[:,'label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest using pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = Pipeline([('tfidf', TfidfVectorizer()), ('rf', RandomForestClassifier())])\n",
    "rf.fit(X1_train, Y1_train)\n",
    "y_pred = rf.predict(X1_test)\n",
    "print(pd.crosstab(Y1_test,y_pred,rownames=['Actual'],colnames=['Predicted']))\n",
    "print(classification_report(Y1_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf.predict([\"\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
